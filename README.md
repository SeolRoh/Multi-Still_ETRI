# Multi-Still: A lightweight Multi-modal Cross Attention Knowledge Distillation method for real-time Emotion Recognition

## ì œ2íšŒ ETRI íœ´ë¨¼ì´í•´ ì¸ê³µì§€ëŠ¥ ë…¼ë¬¸ê²½ì§„ëŒ€íšŒ(2023)
### ë³¸ ëŒ€íšŒëŠ” í•œêµ­ì „ìží†µì‹ ì—°êµ¬ì›(ETRI)ì´ ì£¼ìµœí•˜ê³  ê³¼í•™ê¸°ìˆ ì •ë³´í†µì‹ ë¶€ì™€ êµ­ê°€ê³¼í•™ê¸°ìˆ ì—°êµ¬íšŒ(NST)ê°€ í›„ì›í•©ë‹ˆë‹¤


> ðŸ˜Š ëŒ€íšŒ ì†Œê°œ
+ ì¸ê°„ê³¼ êµê°í•  ìˆ˜ ìžˆëŠ” ì¸ê³µì§€ëŠ¥ì„ êµ¬í˜„í•˜ê¸° ìœ„í•œ ëª©ì ìœ¼ë¡œ ê°œìµœ ë˜ì—ˆìŠµë‹ˆë‹¤.
+ ì‚¬ëžŒì˜ í–‰ë™ê³¼ ê°ì •ì„ ì´í•´í•˜ëŠ” ê¸°ìˆ  ì—°êµ¬ë¥¼ ê°€ëŠ¥í† ë¡ í•˜ê¸°ìœ„í•´ êµ¬ì¶•í•œ ë°ì´í„°ì…‹ì„ í™œìš©í•˜ì—¬ íœ´ë¨¼ì´í•´ ì¸ê³µì§€ëŠ¥ ê¸°ìˆ  ì—°êµ¬ë¥¼ í™•ì‚°ì‹œí‚¤ê³ ìž í•©ë‹ˆë‹¤.
+ ì´ì— ì°½ì˜ì ì€ ì—°êµ¬ë¥¼ ë°œêµ´í•˜ê³ ìž í•©ë‹ˆë‹¤.

> ðŸ˜Š ì£¼ìµœ/ì£¼ê´€
+ ì£¼ìµœ : í•œêµ­ì „ìží†µì‹ ì—°êµ¬ì› (ETRI)
+ í›„ì› : ê³¼í•™ì •ë³´ê¸°ìˆ í†µì‹ ë¶€, êµ­ê°€ê³¼í•™ê¸°ìˆ ì—°êµ¬íšŒ (NST)
+ ìš´ì˜ : ì¸ê³µì§€ëŠ¥íŒ©í† ë¦¬ (AIFactory)

> ðŸ˜Š ë…¼ë¬¸ ì£¼ì œ
+ ë©€í‹°ëª¨ë‹¬ ê°ì • ë°ì´í„°ì…‹ í™œìš© ê°ì • ì¸ì‹ ê¸°ìˆ  ë¶„ì•¼
+ ë…¼ë¬¸ì£¼ì œ:  Emotion Recognition in Conversation (ERC)ë¶„ì•¼
+ Multi-Still: ì‹¤ì‹œê°„ ê°ì • ì¸ì‹ì„ ìœ„í•œ ê²½ëŸ‰í™”ëœ ë©€í‹°ëª¨ë‹¬ êµì°¨ ì–´í…ì…˜ ì§€ì‹ ì¦ë¥˜ ë°©ë²•

      
      * Emotion Recognition in Conversationì´ëž€?
      ë‘ ëª… ì´ìƒì˜ ì°¸ì—¬ìž ê°„ì˜ ëŒ€í™”(dialogue)ê³¼ì •ì—ì„œ ëŒ€í™” ì°¸ì—¬ìžì˜ ê°ì •ì„ ì¸ì‹ ë˜ëŠ” ì˜ˆì¸¡í•˜ê¸° ìœ„í•œ ê°ì •ì¸ì‹ ì—°êµ¬ë¶„ì•¼ìž…ë‹ˆë‹¤.
      

> ðŸ˜Š í™œìš© ë°ì´í„°: ETRI í•œêµ­ì–´ ê°ì • ë°ì´í„°ì…‹ í™œìš© ì—°êµ¬

- ðŸ“  [KEMDy19 (ì„±ìš° ëŒ€ìƒ ìƒí™©ê·¹) ë°ì´í„°ì…‹](https://nanum.etri.re.kr/share/kjnoh/KEMDy19?lang=ko_KR)
- ðŸ“  [KEMDy20 (ì¼ë°˜ì¸ ëŒ€ìƒ ìžìœ ë°œí™”) ë°ì´í„°ì…‹](https://nanum.etri.re.kr/share/kjnoh/KEMDy20?lang=ko_KR)


> ðŸ˜Š Environment
```
python version : python3.9
OS type : WSL
requires packages: {
      'numpy==1.22.3',
      'pandas==1.4.2',
      'torch==1.11.0+cu113',
      'torchaudio==0.11.0+cu113',
      'scikit-learn',
      'transformers==4.18.0',
      'tokenizers==0.12.1',
      'soundfile==0.10.3.post1'
}
```

> ðŸ˜Š Docker container run
```bash
docker container run -d -it --name multi_still --gpus all python:3.9
```

> ðŸ˜Š Environment Setting
```bash
git clone https://github.com/SeolRoh/Multi-Still_ETRI.git
cd Multi-Still_ETRI
bash setup.sh
```
> ðŸ˜Š Preprocessing
```bash
# ë°ì´í„° ì „ì²˜ë¦¬
bash data_preprocessing.sh
```

+ 7ê°€ì§€ ê°ì • ë ˆì´ë¸”ì˜ ë°ì´í„° ë¶ˆê· í˜• ì™„í™” ì „í›„ ë¶„í¬ ë¹„êµ

![](https://github.com/SeolRoh/Multi-Still_ETRI/blob/main/images/datapreprocessing.png)


> ðŸ˜Š Train
```bash
# ë©€í‹°ëª¨ë‹¬ êµì‚¬ ëª¨ë¸ í›ˆë ¨
python train_crossattention.py --model_name multimodal_teacher

# êµì‚¬ëª¨ë¸ì„ í™œìš©í•´, ë°ì´í„°ì…‹ì— ì¦ë¥˜ ë°ì´í„°(Softmax) ì¶”ê°€
#--teacher_name ì˜µì…˜ìœ¼ë¡œ MultiModal êµì‚¬ëª¨ë¸ì˜ ì´ë¦„_epochìˆ˜ë¥¼ ìž…ë ¥í•œë‹¤.
#--data_path ì˜µì…˜ìœ¼ë¡œ softmax ë°ì´í„°ë¥¼ ì¶”ê°€í•  ê¸°ì¡´ ë°ì´í„°ì…‹ì˜ ê²½ë¡œë¥¼ ìž…ë ¥ (ê¸°ë³¸ê°’, "data/train_preprocessed_data.json")
python Distill_knowledge.py --teacher_name multimodal_teacher_epoch4 

# miniconfig.py ë¥¼ ìˆ˜ì •í•´ì„œ Epochë¥¼ í¬í•¨í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ë³€ê²½
# ë©€í‹°ëª¨ë‹¬ í•™ìƒ ëª¨ë¸ ì§€ì‹ì¦ë¥˜ í›ˆë ¨
python KD_train_crossattention.py --model_name multimodal_student
# ë¬¸ìžëª¨ë‹¬ í•™ìƒ ëª¨ë¸ ì§€ì‹ì¦ë¥˜ í›ˆë ¨
python KD_train_crossattention.py --model_name text_student --text_only True 
# ìŒì„±ëª¨ë‹¬ í•™ìƒ ëª¨ë¸ ì§€ì‹ì¦ë¥˜ í›ˆë ¨
python KD_train_crossattention.py --model_name audio_student --audio_only True
```

> ðŸ˜Š Test
```bash
# pt íŒŒì¼ì€ í›ˆë ¨ì˜ 5ë²ˆì§¸ Epochë§ˆë‹¤ ìƒì„±ë¨. (ì˜ˆ: 5, 10, 11....)
# ì—¬ëŸ¬ íŒŒì¼ì„ í…ŒìŠ¤íŠ¸ í•˜ê¸°ìœ„í•´ test_allíŒŒì¼ì— ë³µì‚¬
mkdir ckpt/test_all
cp ckpt/* ckpt/test_all/
python test.py --all

# test.pyì˜ ê²°ê³¼ëŠ” "result_all_model2.csv"ì—ì„œ 
```


> ðŸ˜ Directory
- ì½”ë“œ êµ¬í˜„ì„ ìœ„í•´ì„œëŠ” ETRIì—ì„œ ì œê³µí•˜ëŠ” íŒŒì¼(KEMDy19 & KEMDy20)ê³¼ AI Hub ê°ì • ë°ì´í„° íŒŒì¼ì´ ì•Œë§žì€ ìœ„ì¹˜ì— ìžˆì–´ì•¼í•©ë‹ˆë‹¤.
```
+--Multi-Still_ETRI
      +--KEMDy19
            +--annotation
            +--EDA
            +--TEMP
            +--wav

      +--KEMDy20
            +--annotation
            +--wav
            +--TEMP
            +--IBI
            +--EDA 

      +--models
            +--module_for_clossattention
                  +--MultiheadAttention.py            # ë©€í‹°í—¤ë“œ ì–´í…ì…˜
                  +--PositionalEmbedding.py           # í¬ì§€ì…”ë„ ìž„ë² ë”©
                  +--Transformer.py                   # íŠ¸ëžœìŠ¤í¬ë¨¸
            +--multimodal.py                          # ë©€í‹°ëª¨ë‹¬, encodingì§„í–‰
            +--multimodal_cross_attention.py          # ìŒì„± ë° í…ìŠ¤íŠ¸ë¥¼ í¬í•¨í•œ ë©€í‹°ëª¨ë‹¬ ì¸ì½”ë”© ë° í¬ë¡œìŠ¤ ì–´í…ì…˜

      +--data (data_preprocessing.sh ì‹¤í–‰ í›„ ìƒì„±)
            +--total_data.json                        # ëª¨ë“  ë°ì´í„°ì…‹ì„ ì „ì²˜ë¦¬í•œ íŒŒì¼
            +--preprocessed_data.json                 # ëª¨ë“  ë°ì´í„°ì…‹ì—ì„œ ìŒì„±íŒŒì¼ì´ ì¡´ìž¬í•˜ì§€ ì•ŠëŠ” ë°ì´í„°ë¥¼ ì œê±° í›„, í…ŒìŠ¤íŠ¸ë°ì´í„°ì™€ í›ˆë ¨ ë°ì´í„°ë¥¼ ë¶„ë¦¬í•œ íŒŒì¼
            +--test_preprocessed_data.json            # preprocessed_data.jsonì—ì„œ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì¶”ì¶œí•œ íŒŒì¼
            +--train_preprocessed_data.json           # preprocessed_data.jsonì—ì„œ í›ˆë ¨ ë°ì´í„°ë¥¼ ì¶”ì¶œí•œ íŒŒì¼

      +--ckpt (train_crossattention.py, KD_train_crossattention.py ì‹¤í–‰ í›„ ìƒì„±)
            +--test_all                               # ì—¬ëŸ¬ ëª¨ë¸ë“¤ì„ í•œë²ˆì— í…ŒìŠ¤íŠ¸í•  ë•Œ ë³µì‚¬í•´ì¤„ í´ë”
            +--*.pt                                   # ëª¨ë¸ í›ˆë ¨ í›„, 5ì˜ ë°°ìˆ˜ Epochë§ˆë‹¤ ì €ìž¥ë˜ëŠ” ëª¨ë¸ íŒŒì¼

      +--TOTAL(Data_Preprocessing.sh ì‹¤í–‰ í›„ ìƒì„±)     # ëª¨ë“  ë°ì´í„°ë¥¼ TOTAL í´ë”ì— ë³µì‚¬í•œ í›„, ì „ì²˜ë¦¬ ë° í›ˆë ¨ ì§„í–‰
            +--hidden_states                          # í›ˆë ¨ ë° ì¶”ë¡ ì„ ë¹¨ë¦¬ ì§„í–‰í•˜ê¸° ìœ„í•´, ë¯¸ë¦¬ í›ˆë ¨ëœ Wav2Vec2ëª¨ë¸ì—ì„œ ì¸ì½”ë”©í•œ ê²°ê³¼ë¥¼ ë¯¸ë¦¬ ì €ìž¥í•˜ì—¬ í™œìš©.

      +--setup.sh                         # update, upgrade ë° ëª¨ë¸ ìƒì„± ì‹œ í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜

      +--data_preprocessing.sh            # ë°ì´í„° ì „ì²˜ë¦¬ ë° í›ˆë ¨, í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ë¶„ë¦¬ ì €ìž¥

      +--config.py                        # êµì‚¬ëª¨ë¸ í›ˆë ¨ì‹œ í•„ìš”í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì •ì˜

      +--Data_Balancing.py                # ë°ì´í„°ì…‹ ì „ì²˜ë¦¬ ë° í›ˆë ¨, í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ë¶„ë¦¬ ì €ìž¥

      +--Distill_knowledge.py             # í›ˆë ¨ëœ êµì‚¬ ëª¨ë¸ì„ ì´ìš©í•´, ë°ì´í„° ì…‹ì— ì¦ë¥˜ëœ ì§€ì‹(Softmax) ë°ì´í„° ì¶”ê°€ì €ìž¥ 

      +--KD_train_crossattention.py       # ì¦ë¥˜ëœ ì§€ì‹ì„ í†µí•´, í•™ìƒëª¨ë¸ì„ í›ˆë ¨

      +--KEMDy_preprocessing.py           # ëª¨ë“  ë°ì´í„°ë¥¼ TOTAL í´ë”ë¡œ ì´ë™ í›„, ë°ì´í„°ì…‹ìœ¼ë¡œ ê°€ê³µ

      +--merdataset.py                    # ë°ì´í„°ë¥¼ ì½ê³ , ëª¨ë¸ í›ˆë ¨ì‹œ ë°ì´í„°ë¥¼ ì œê³µí•´ì£¼ëŠ” DataLoader íŒŒì¼

      +--mini_config.py                   # í•™ìƒëª¨ë¸ í›ˆë ¨ì‹œ í•„ìš”í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì •ì˜

      +--test.py                          # ckpt í´ë”ì— ì €ìž¥ë˜ì–´ ìžˆëŠ” ëª¨ë¸ì„ í…ŒìŠ¤íŠ¸
      
      +--train_crossattention.py          # í¬ë¡œìŠ¤ ì–´í…ì…˜ ê¸°ë°˜ êµì‚¬ëª¨ë¸ì„ í›ˆë ¨í•˜ëŠ” íŒŒì¼

      +--utils.py                         # logger, get_params ë“± ëª¨ë¸ ì •ë³´ë¥¼ ë³¼ ìˆ˜ ìžˆëŠ” ê¸°ëŠ¥ í¬í•¨
```

> ðŸ˜† Base Model

Encoder | Architecture | pretrained-weights
:------------: | :-------------: | :-------------:
Audio Encoder | pretrained Wav2Vec 2.0 | kresnik/wav2vec2-large-xlsr-korean
Text Encoder | pretrained Electra | monologg/koelectra-base 

> ðŸ˜ƒ Arguments
- train_crossattention.py

Arguments | Description
:------------: | :-------------:
--epochs | ëª¨ë¸ ë°˜ë³µ í›ˆë ¨ ìˆ˜
--batch | ë°ì´í„° batch ì‚¬ì´ì¦ˆ
--shuffle | í›ˆë ¨ ë°ì´í„°ì˜ shuffle ì—¬ë¶€
--lr | í›ˆë ¨ì‹œ ì‚¬ìš©í•  Learning rate ê°’
--cuda | ì‚¬ìš©í•  GPU ì •ì˜ (default="cuda:0")
--save | ëª¨ë¸ì˜ ì €ìž¥ ì—¬ë¶€
--model_name | ëª¨ë¸ ì €ìž¥ì‹œ, ì €ìž¥í•  ëª¨ë¸ì˜ ì´ë¦„
--text_only | í…ìŠ¤íŠ¸ ë°ì´í„° ë° ì¸ì½”ë”ë§Œ ì‚¬ìš©í•´ì„œ í›ˆë ¨
--audio_only | ì˜¤ë””ì˜¤ ë°ì´í„° ë° ì¸ì½”ë”ë§Œ ì‚¬ìš©í•´ì„œ í›ˆë ¨

- Distill_knowledge.py

Arguments | Description
:------------: | :-------------:
--cuda | ì‚¬ìš©í•  GPU ì •ì˜ (default="cuda:0")
--teacher_name | ì§€ì‹ ì¦ë¥˜í•  ì €ìž¥ëœ ëª¨ë¸ì˜ ì´ë¦„
--data_path | ì§€ì‹ ì¦ë¥˜í•  ë°ì´í„°ì˜ ê²½ë¡œ

- KD_train_crossattention.py

Arguments | Description
:------------: | :-------------:
--epochs | ëª¨ë¸ ë°˜ë³µ í›ˆë ¨ ìˆ˜
--batch | ë°ì´í„° batch ì‚¬ì´ì¦ˆ
--shuffle | í›ˆë ¨ ë°ì´í„°ì˜ shuffle ì—¬ë¶€
--lr | í›ˆë ¨ì‹œ ì‚¬ìš©í•  Learning rate ê°’
--cuda | ì‚¬ìš©í•  GPU ì •ì˜ (default="cuda:0")
--save | ëª¨ë¸ì˜ ì €ìž¥ ì—¬ë¶€
--model_name | ëª¨ë¸ ì €ìž¥ì‹œ, ì €ìž¥í•  ëª¨ë¸ì˜ ì´ë¦„
--text_only | í…ìŠ¤íŠ¸ ë°ì´í„° ë° ì¸ì½”ë”ë§Œ ì‚¬ìš©í•´ì„œ í›ˆë ¨
--audio_only | ì˜¤ë””ì˜¤ ë°ì´í„° ë° ì¸ì½”ë”ë§Œ ì‚¬ìš©í•´ì„œ í›ˆë ¨

- test.py

Arguments | Description
:------------: | :-------------:
--batch | ë°ì´í„° batch ì‚¬ì´ì¦ˆ
--cuda | ì‚¬ìš©í•  GPU ì •ì˜ (default="cuda:0")
--model_name | í…ŒìŠ¤íŠ¸í•  ëª¨ë¸ì˜ ì´ë¦„(ì˜ˆ: ckpt/[ëª¨ë¸ì´ë¦„].pt)
--all | "ckpt/test_all" ê²½ë¡œ ì•ˆì— ìžˆëŠ” ëª¨ë“  ëª¨ë¸ í…ŒìŠ¤íŠ¸


> ðŸ˜€ Model Architecture
- `Multi-Still` ê²½ëŸ‰í™” ê¸°ìˆ  ì¤‘ í•˜ë‚˜ì¸ ì§€ì‹ ì¦ë¥˜ (Knowledge Distillation)ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¤ì‹œê°„ ê°ì •ì¸ì‹ì„ ìœ„í•œ ë©€í‹°ëª¨ë‹¬ êµ¬ì¡°ë¥¼ ê²½ëŸ‰í™”í•˜ëŠ” ë°©ë²• 
- ðŸ‘©â€ðŸ«âž¡ðŸ‘¨â€ðŸ’» Muti-Still Architecture
![](https://github.com/SeolRoh/Multi-Still_ETRI/blob/main/images/structure.png)
- ðŸ‘©â€ðŸ« Teacher Model
![](https://github.com/SeolRoh/Multi-Still_ETRI/blob/main/images/teacher.png)

> ðŸ˜† Experiments 
+ í…ìŠ¤íŠ¸ ëª¨ë¸(KoELECTRA)
+ ì˜¤ë””ì˜¤ ëª¨ë¸(WAV2VEC 2.0)
+ êµì‚¬ ëª¨ë¸(Multimodal Cross-Attention)
+ í•™ìƒëª¨ë¸((a)Text-OnlyStudent, (b)Audio-OnlyStudent, (c)MultimodalStudent)

![](https://github.com/SeolRoh/Multi-Still_ETRI/blob/main/images/Experiments.png)


> ðŸ™‚ References

+ [1]  Xu, Peng, Xiatian Zhu, and David A. Clifton. "Multimodal learning with transformers: A survey." arXiv preprint arXiv:2206.06488 (2022).

+ [2] Tao, Jiang, Zhen Gao, and Zhaohui Guo. "Training Vision Transformers in Federated Learning with Limited Edge-Device Resources." Electronics 11.17, 2638. (2022).

+ [3] Hinton, Geoffrey, Oriol Vinyals, and Jeff Dean. â€Distilling the knowledge in a neural network.â€ arXiv preprint arXiv:1503.02531 2.7 (2015).

+ [4] Gou, J., Yu, B., Maybank, S. J., & Tao, D. â€œKnowledge distillation: A surveyâ€. International Journal of Computer Vision, 129, 1789-1819. (2021).

+ [5] K. J. Noh and H. Jeong, â€œKEMDy19,â€ https://nanum.etri.re.kr/share/kjnoh/KEMDy19?lang=ko_KR.

+ [6] Noh, K.J.; Jeong, C.Y.; Lim, J.; Chung, S.; Kim, G.; Lim, J.M.; Jeong, H. Multi-Path and Group-Loss-Based Network for Speech Emotion Recognition in Multi-Domain Datasets. Sensors 2021, 21, 1579. https://doi.org/10.3390/s21051579. 

+ [7] K. J. Noh and H. Jeong, â€œKEMDy20,â€ https://nanum.etri.re.kr/share/kjnoh/KEMDy20?lang=ko_KR.

+ [8] Thabtah, Fadi, et al. "Data imbalance in classification: Experimental evaluation." Information Sciences 513 : 429-441 (2020).

+ [9] Park Jangwon, â€œKoELECTRA: Pretrained ELECTRA Model for Koreanâ€https://github.com/monologg/KoELECT RA (2020).

+ [10] Baevski, Alexei, et al. â€œwav2vec 2.0: A framework for self-supervised learning of speech representationsâ€. Advances in Neural Information Processing Systems, 33, 12449-12460, (2020).


> ðŸ™‚ Contact
+ Hyun-Ki Jo : jhk1132@khu.ac.kr
+ Yu-Ri Seo : yuri0329@khu.ac.kr
+ Seol Roh : seven800@khu.ac.kr
